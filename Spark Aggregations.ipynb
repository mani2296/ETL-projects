{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7341d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58d0b468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(r\"C:\\Users\\manig\\Documents\\Datasets\\spark-data\\data\\retail-data\\all\\*.csv\")\\\n",
    ".coalesce(5)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dftable\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98058124",
   "metadata": {},
   "source": [
    "# Count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f073385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count is a type of action which does basic aggregation\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61d97ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#count as transformation instead of action. count(*), Spark will count null values wheraes counting an individual column, \n",
    "#Spark will not count the null values\n",
    "from pyspark.sql.functions import count\n",
    "df.select(count(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4aa232d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"select count(StockCode) from dftable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6e8cf721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536641"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding count without duplicate columns\n",
    "df.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac44ae7",
   "metadata": {},
   "source": [
    "# countDistinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b11dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to find number of unique groups \n",
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "06a1167e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"select count(Distinct StockCode) from dftable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b65208ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#times when u want count to an approximation to a certain degree of accuracy.approx_count_distinct takes another parameter \n",
    "#with which you can specify the maximum estimation error allowed\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b6b74db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"SELECT approx_count_distinct(StockCode, 0.1) FROM DFTABLE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2d64fd",
   "metadata": {},
   "source": [
    "# first(), last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa2bd092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------+\n",
      "|first(StockCode, false)|last(StockCode, false)|\n",
      "+-----------------------+----------------------+\n",
      "|                 85123A|                 22138|\n",
      "+-----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#You can get the first and last values from a DataFrame by using this \n",
    "from pyspark.sql.functions import first, last\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd2855d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------+\n",
      "|first(StockCode, false)|last(StockCode, false)|\n",
      "+-----------------------+----------------------+\n",
      "|                 85123A|                 22138|\n",
      "+-----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"SELECT first(StockCode),last(StockCode) FROM DFTABLE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4599131",
   "metadata": {},
   "source": [
    "# min(), max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a59176c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To extract the minimum and maximum values from a DataFrame\n",
    "from pyspark.sql.functions import min, max\n",
    "df.selectExpr(\"min(Quantity)\", \"max(Quantity)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1b6ace2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"SELECT min(Quantity),max(Quantity) FROM DFTABLE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12dffbd",
   "metadata": {},
   "source": [
    "# sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d709cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to add all the values in a row using the sum\n",
    "from pyspark.sql.functions import sum\n",
    "df.selectExpr(\"sum(Quantity)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a18cc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"SELECT sum(Quantity) FROM DFTABLE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faba4c46",
   "metadata": {},
   "source": [
    "# sumDistinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1358e310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to sum a distinct set of values\n",
    "df.select(sumDistinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b320f44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"SELECT sum(distinct Quantity) FROM DFTABLE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c61a652",
   "metadata": {},
   "source": [
    "# avg() or mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fec311b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+----------------+\n",
      "|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "|                      9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#various ways of finding average\n",
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "df.select(\n",
    "count(\"Quantity\").alias(\"total_transactions\"),\n",
    "sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n",
    ".selectExpr(\n",
    "\"total_purchases/total_transactions\",\n",
    "\"avg_purchases\",\n",
    "\"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69c5d94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+----------------+\n",
      "|  sum_div_count_|   avg_purchases|  mean_purchases|\n",
      "+----------------+----------------+----------------+\n",
      "|9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+----------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"SELECT sum(Quantity)/count(Quantity) as sum_div_count_,avg(Quantity) as avg_purchases,mean(Quantity) as mean_purchases FROM DFTABLE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143de1a3",
   "metadata": {},
   "source": [
    "# collect_set(), collect_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6a235e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Aggregating to Complex Types -we can collect a list of values present in a given column or only the unique values \n",
    "#by collecting to a set\n",
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "df.select(collect_set(\"Country\"), collect_list(\"Country\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "913a941e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select collect_set(Country),collect_list(Country) from dftable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3266c876",
   "metadata": {},
   "source": [
    "# groupBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "889288d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "+---------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Grouping - First we specify the column(s) on which we would like to group, and then we specify the aggregation(s).\n",
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5720517f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|InvoiceNo|CustomerId|total_count|\n",
      "+---------+----------+-----------+\n",
      "|   536846|     14573|         76|\n",
      "|   537026|     12395|         12|\n",
      "|   537883|     14437|          5|\n",
      "|   538068|     17978|         12|\n",
      "|   538279|     14952|          7|\n",
      "+---------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").agg(expr(\"count(1) as total_count\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43ac3cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----+\n",
      "|InvoiceNo|quan|quan1|\n",
      "+---------+----+-----+\n",
      "|   536596|   6|    6|\n",
      "|   536938|  14|   14|\n",
      "|   537252|   1|    1|\n",
      "|   537691|  20|   20|\n",
      "|   538041|   1|    1|\n",
      "+---------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Grouping with Expressions - function. Rather than passing function as an expression into a select statement, \n",
    "#we specify it within agg\n",
    "from pyspark.sql.functions import count\n",
    "df.groupBy(\"InvoiceNo\").agg(count(\"Quantity\").alias(\"quan\"),expr(\"count(Quantity) as quan1\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "137a86ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------------+\n",
      "|InvoiceNo|sum(Quantity)|max(UnitPrice)|\n",
      "+---------+-------------+--------------+\n",
      "|   536596|            9|         19.95|\n",
      "|   536938|          464|         10.95|\n",
      "|   537252|           31|          0.85|\n",
      "|   537691|          163|          9.95|\n",
      "|   538041|           30|           0.0|\n",
      "+---------+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(sum(\"Quantity\"),max(\"UnitPrice\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "385af947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+\n",
      "|InvoiceNo|CustomerId|count(1)|\n",
      "+---------+----------+--------+\n",
      "|   536846|     14573|      76|\n",
      "|   537026|     12395|      12|\n",
      "|   537883|     14437|       5|\n",
      "|   538068|     17978|      12|\n",
      "|   538279|     14952|       7|\n",
      "+---------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql - group by should be specified after where or from clause\n",
    "spark.sql(\"SELECT InvoiceNo,CustomerId,count(*) FROM dfTable GROUP BY InvoiceNo, CustomerId\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f935a59",
   "metadata": {},
   "source": [
    "# window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0f6e863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|   date_dt|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#window\n",
    "from pyspark.sql.functions import col, to_date\n",
    "dfWithDate = df.withColumn(\"date_dt\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate_sql\")\n",
    "dfWithDate.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cd69282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "windowSpec = Window\\\n",
    ".partitionBy(\"CustomerId\", \"date_dt\")\\\n",
    ".orderBy(desc(\"Quantity\"))\\\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5bd1ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d614c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dense_rank, rank\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d97fc002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "dfp=dfWithDate.where(\"CustomerId IS NOT NULL\").where(\"date_dt IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    ".select(\n",
    "col(\"CustomerId\"),\n",
    "col(\"date_dt\"),\n",
    "col(\"Quantity\"),\n",
    "purchaseRank.alias(\"quantityRank\"),\n",
    "purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "maxPurchaseQuantity.alias(\"maxPurchaseQuantity\"),sum(\"Quantity\").over(windowSpec).alias(\"running_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "13916844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+-------------------+-------------+\n",
      "|CustomerId|date_dt   |Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|running_total|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+-------------+\n",
      "|12347     |2010-12-07|36      |1           |1                |36                 |36           |\n",
      "|12347     |2010-12-07|30      |2           |2                |36                 |66           |\n",
      "|12347     |2010-12-07|24      |3           |3                |36                 |90           |\n",
      "|12347     |2010-12-07|12      |4           |4                |36                 |102          |\n",
      "|12347     |2010-12-07|12      |4           |4                |36                 |114          |\n",
      "|12347     |2010-12-07|12      |4           |4                |36                 |126          |\n",
      "|12347     |2010-12-07|12      |4           |4                |36                 |138          |\n",
      "|12347     |2010-12-07|12      |4           |4                |36                 |150          |\n",
      "|12347     |2010-12-07|12      |4           |4                |36                 |162          |\n",
      "|12347     |2010-12-07|12      |4           |4                |36                 |174          |\n",
      "|12347     |2010-12-07|12      |4           |4                |36                 |186          |\n",
      "|12347     |2010-12-07|12      |4           |4                |36                 |198          |\n",
      "|12347     |2010-12-07|12      |4           |4                |36                 |210          |\n",
      "|12347     |2010-12-07|12      |4           |4                |36                 |222          |\n",
      "|12347     |2010-12-07|12      |4           |4                |36                 |234          |\n",
      "|12347     |2010-12-07|12      |4           |4                |36                 |246          |\n",
      "|12347     |2010-12-07|6       |17          |5                |36                 |252          |\n",
      "|12347     |2010-12-07|6       |17          |5                |36                 |258          |\n",
      "|12347     |2010-12-07|6       |17          |5                |36                 |264          |\n",
      "|12347     |2010-12-07|6       |17          |5                |36                 |270          |\n",
      "+----------+----------+--------+------------+-----------------+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfp.show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62169cf",
   "metadata": {},
   "source": [
    "# rollup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3aedefad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------+\n",
      "|   date_dt|  Country|total_quantity|\n",
      "+----------+---------+--------------+\n",
      "|      null|     null|       1739779|\n",
      "|2010-12-01|     null|         24032|\n",
      "|2010-12-01|   Norway|          1852|\n",
      "|2010-12-01|Australia|           107|\n",
      "|2010-12-01|  Germany|           117|\n",
      "+----------+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#rollup : rollup is a multidimensional aggregation that performs a variety of group-by style calculations\n",
    "#it does not include rows if left column in rollup  has a null value but includes if both rollup column has null value or if right \n",
    "#col alone has null value\n",
    "dfNoNull = dfWithDate.na.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")\n",
    "from pyspark.sql.functions import sum\n",
    "rolledUpDF = dfNoNull.rollup(\"date_dt\", \"Country\").agg(sum(\"Quantity\").alias(\"total_quantity\")).orderBy(\"date_dt\")\n",
    "rolledUpDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a674a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------+\n",
      "|Date|Country|total_quantity|\n",
      "+----+-------+--------------+\n",
      "|null|   null|       1739779|\n",
      "+----+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#where you see the null values is where youâ€™ll find the grand totals\n",
    "#null in both rollup columns specifies the grand total across both of those columns\n",
    "rolledUpDF.where(\"Country IS NULL and Date is null\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99cfdd9",
   "metadata": {},
   "source": [
    "# cube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5a80a116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-------------+\n",
      "|date_dt|        Country|sum(Quantity)|\n",
      "+-------+---------------+-------------+\n",
      "|   null|Channel Islands|         2260|\n",
      "|   null|            USA|          897|\n",
      "|   null|          Spain|         7906|\n",
      "|   null|         Norway|        11001|\n",
      "|   null| Czech Republic|          285|\n",
      "+-------+---------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cube:Irrespective of rollup it includes all values\n",
    "dfNoNull.cube(\"date_dt\", \"Country\").agg(sum(\"Quantity\"))\\\n",
    ".orderBy(\"date_dt\").show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
