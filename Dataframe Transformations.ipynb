{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bc8dc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "db48797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading csv file\n",
    "df=spark.read.format(\"csv\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".option(\"inferSchema\",\"true\")\\\n",
    ".load(\"\\data\\retail-data\\by-day\\2010-12-01.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae52115",
   "metadata": {},
   "source": [
    "# creating temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "affded0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|     InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|01-12-2010 08:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|01-12-2010 08:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"dftable\")\n",
    "spark.sql(\"select * from dftable\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fb16b4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144683d6",
   "metadata": {},
   "source": [
    "# working with strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f784a2f",
   "metadata": {},
   "source": [
    "# lit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afeaee6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "|  5|five|5.0|\n",
      "+---+----+---+\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "+---+----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Converting to Spark Types.lit function converts a type in another language to its correspnding Spark representation.\n",
    "from pyspark.sql.functions import lit\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc43972b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "|  5|five|5.0|\n",
      "+---+----+---+\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "+---+----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"select 5,'five',5.0 from dftable\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9733d0ef",
   "metadata": {},
   "source": [
    "# Working with Booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "164b60a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|InvoiceNo|         Description|\n",
      "+---------+--------------------+\n",
      "|   536366|HAND WARMER UNION...|\n",
      "|   536366|HAND WARMER RED P...|\n",
      "|   536367|ASSORTED COLOUR B...|\n",
      "|   536367|POPPY'S PLAYHOUSE...|\n",
      "|   536367|POPPY'S PLAYHOUSE...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.where(\"InvoiceNo != 536365\")\\\n",
    ".select(\"InvoiceNo\", \"Description\")\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "676c5200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|InvoiceNo|         Description|\n",
      "+---------+--------------------+\n",
      "|   536366|HAND WARMER UNION...|\n",
      "|   536366|HAND WARMER RED P...|\n",
      "|   536367|ASSORTED COLOUR B...|\n",
      "|   536367|POPPY'S PLAYHOUSE...|\n",
      "|   536367|POPPY'S PLAYHOUSE...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"select InvoiceNo,Description from dftable where InvoiceNo!=536365\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ab1ab4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#applying multiple filters\n",
    "from pyspark.sql.functions import instr\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(df.Description, \"POSTAGE\") >= 1\n",
    "df.where(col(\"StockCode\").contains(\"DOT\")).where(priceFilter | descripFilter).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23203446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+----------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|     InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+----------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|01-12-2010 14:32|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|01-12-2010 17:06|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+----------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#applying multiple filters in single where clause\n",
    "df.where(\"StockCode like '%DOT%' and (UnitPrice>600 or Description like '%POSTAGE%')\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19ee47d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+----------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|     InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+----------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|01-12-2010 14:32|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|01-12-2010 17:06|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+----------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"SELECT * FROM dfTable WHERE StockCode in ('DOT') AND (UnitPrice > 600 OR Description like '%POSTAGE%')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccdcd3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|unitPrice|isExpensive|\n",
      "+---------+-----------+\n",
      "|   569.77|       true|\n",
      "|   607.49|       true|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DOTCodeFilter = col(\"StockCode\") == \"DOT\"\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter))\\\n",
    ".where(\"isExpensive\")\\\n",
    ".select(\"unitPrice\", \"isExpensive\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad3f5a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+-----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|     InvoiceDate|UnitPrice|CustomerID|       Country|isExpensive|\n",
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+-----------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|01-12-2010 08:26|     2.55|     17850|United Kingdom|      false|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|01-12-2010 08:26|     3.39|     17850|United Kingdom|      false|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|01-12-2010 08:26|     2.75|     17850|United Kingdom|      false|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|01-12-2010 08:26|     3.39|     17850|United Kingdom|      false|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|01-12-2010 08:26|     3.39|     17850|United Kingdom|      false|\n",
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#applying conditions single expression\n",
    "df.withColumn(\"isExpensive\", expr(\"StockCode =='DOT' and (UnitPrice > 600 or instr(Description, 'POSTAGE') >= 1)\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dff22872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|UnitPrice|isExpensive|\n",
      "+---------+-----------+\n",
      "|   569.77|       true|\n",
      "|   607.49|       true|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"SELECT UnitPrice, (StockCode = 'DOT' AND (UnitPrice > 600 OR instr(Description, 'POSTAGE') >= 1)) as isExpensive FROM dfTable WHERE (StockCode = 'DOT' AND(UnitPrice > 600 OR instr(Description, 'POSTAGE') >= 1))\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c34d67c",
   "metadata": {},
   "source": [
    "# Working with numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "042c8e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, pow\n",
    "fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d397285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"select CustomerId,pow(Quantity*UnitPrice,2)+5 as realQuantity from dftable\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb0b85a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "\"CustomerId\",\n",
    "\"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b41321d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"select CustomerId,pow(Quantity*UnitPrice,2.0)+5 as realQuantity from dftable\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eded322",
   "metadata": {},
   "source": [
    "# round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aea2cab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|round|bround|\n",
      "+-----+------+\n",
      "|2.557|   2.0|\n",
      "|2.557|   2.0|\n",
      "+-----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#rounding of\n",
    "from pyspark.sql.functions import lit, round, bround\n",
    "df.select(round(lit(2.55678999999),3).alias(\"round\"), bround(lit(2.5)).alias(\"bround\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b8eee84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|rounded_of|UnitPrice|\n",
      "+----------+---------+\n",
      "|       2.6|     2.55|\n",
      "|       3.4|     3.39|\n",
      "|       2.8|     2.75|\n",
      "|       3.4|     3.39|\n",
      "|       3.4|     3.39|\n",
      "+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"round(UnitPrice, 1) as rounded_of\", \"UnitPrice\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6f17924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+--------------+----------+\n",
      "|round(2.5, 0)|CEIL(2.5)|bround(2.5, 0)|FLOOR(2.5)|\n",
      "+-------------+---------+--------------+----------+\n",
      "|            3|        3|             2|         2|\n",
      "|            3|        3|             2|         2|\n",
      "+-------------+---------+--------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#round and bround does similiar functions like ceil and floor respectively\n",
    "spark.sql(\"select round(2.5),ceil(2.5),bround(2.5),floor(2.5) from dftable\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "722f27b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+----------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|     InvoiceDate|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+----------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|            3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128|            null| 4.151946589446603|15661.388719512195|          null|\n",
      "| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|            null|15.638659854603892|1854.4496996893627|          null|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|01-12-2010 08:26|               0.0|             12431|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|01-12-2010 17:35|            607.49|             18229|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+----------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to know about statistics of each columns use describe\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effaba6f",
   "metadata": {},
   "source": [
    "# monotonically_increasing_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e15b2b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|row_id|\n",
      "+------+\n",
      "|     1|\n",
      "|     2|\n",
      "+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df.selectExpr(\"monotonically_increasing_id()+1 as row_id\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "636276f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select row_number() over(order by 1) as id from dfTable\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85f2cc4",
   "metadata": {},
   "source": [
    "# Working with strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d134fdc",
   "metadata": {},
   "source": [
    "# 1) initcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3dfce66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|Description                        |\n",
      "+-----------------------------------+\n",
      "|White Hanging Heart T-light Holder |\n",
      "|White Metal Lantern                |\n",
      "|Cream Cupid Hearts Coat Hanger     |\n",
      "|Knitted Union Flag Hot Water Bottle|\n",
      "|Red Woolly Hottie White Heart.     |\n",
      "+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#capitalize every first word\n",
    "from pyspark.sql.functions import initcap\n",
    "df.select(initcap(\"Description\").alias(\"Description\")).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4afde27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|initcap(Description)               |\n",
      "+-----------------------------------+\n",
      "|White Hanging Heart T-light Holder |\n",
      "|White Metal Lantern                |\n",
      "|Cream Cupid Hearts Coat Hanger     |\n",
      "|Knitted Union Flag Hot Water Bottle|\n",
      "|Red Woolly Hottie White Heart.     |\n",
      "+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"SELECT initcap(Description) FROM dfTable\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c653a2d2",
   "metadata": {},
   "source": [
    "# 2) lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e6685f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|         Description|  lower(Description)|  upper(Description)|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|WHITE HANGING HEA...|white hanging hea...|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN| white metal lantern| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#converting to uppercase and lowercase\n",
    "from pyspark.sql.functions import lower, upper\n",
    "df.selectExpr(\"Description\",\"lower(Description)\",\"upper(Description)\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc76cb9d",
   "metadata": {},
   "source": [
    "# 3) concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9cfb2a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|upper(Description)                 |concat(Description, ,, Stockcode)         |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |WHITE HANGING HEART T-LIGHT HOLDER,85123A |\n",
      "|WHITE METAL LANTERN                |WHITE METAL LANTERN,71053                 |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |CREAM CUPID HEARTS COAT HANGER,84406B     |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|KNITTED UNION FLAG HOT WATER BOTTLE,84029G|\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |RED WOOLLY HOTTIE WHITE HEART.,84029E     |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"upper(Description)\",\"concat(Description,',',Stockcode)\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aa465b",
   "metadata": {},
   "source": [
    "# 4) trim, pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d09e08a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+--------+--------+\n",
      "| ltrim| rtrim| trim|      lp|      rp|\n",
      "+------+------+-----+--------+--------+\n",
      "|HELLO | HELLO|HELLO|   HELLO|HELLO   |\n",
      "|HELLO | HELLO|HELLO|   HELLO|HELLO   |\n",
      "+------+------+-----+--------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#adding or removing spaces around a string\n",
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "df.select(\n",
    "ltrim(lit(\"   HELLO \")).alias(\"ltrim\"),\n",
    "rtrim(lit(\" HELLO \")).alias(\"rtrim\"),\n",
    "trim(lit(\" HELLO \")).alias(\"trim\"),\n",
    "lpad(lit(\"HELLO\"), 8, \" \").alias(\"lp\"),\n",
    "rpad(lit(\"HELLO\"),8, \" \").alias(\"rp\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a889733f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+----+----------+\n",
      "|     ltrim|     rtrim|     trim|lpad|      rpad|\n",
      "+----------+----------+---------+----+----------+\n",
      "|HELLLOOOO | HELLLOOOO|HELLLOOOO| HEL|HELLOOOO  |\n",
      "|HELLLOOOO | HELLLOOOO|HELLLOOOO| HEL|HELLOOOO  |\n",
      "+----------+----------+---------+----+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT ltrim(' HELLLOOOO ') as ltrim,rtrim(' HELLLOOOO ') as rtrim,trim(' HELLLOOOO ') as trim,lpad('HELLOOOO ', 3, ' ') as lpad,\\\n",
    "rpad('HELLOOOO ', 10, ' ') as rpad FROM dfTable\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3919e8",
   "metadata": {},
   "source": [
    "# Working with Regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e789c00d",
   "metadata": {},
   "source": [
    "# 1) regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "090073db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         color_clean|         Description|\n",
      "+--------------------+--------------------+\n",
      "|COLOR HANGING HEA...|WHITE HANGING HEA...|\n",
      "| COLOR METAL LANTERN| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#repacing the values\n",
    "#syntax regexp_replace(column,regex_string_to_replace,replacement_string) \n",
    "from pyspark.sql.functions import regexp_replace\n",
    "df.selectExpr(\"regexp_replace(Description,'BLACK|WHITE|RED|GREEN|BLUE','COLOR') as color_clean\",\"Description\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fc4c4797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+\n",
      "|InvoiceDate     |reg_exp   |\n",
      "+----------------+----------+\n",
      "|01-12-2010 08:26|2010 08:26|\n",
      "|01-12-2010 08:26|2010 08:26|\n",
      "|01-12-2010 08:26|2010 08:26|\n",
      "|01-12-2010 08:26|2010 08:26|\n",
      "|01-12-2010 08:26|2010 08:26|\n",
      "+----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"InvoiceDate\",\"trim(regexp_replace(InvoiceDate,'([0-9]+)(-){1}','')) as reg_exp\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1fd53a",
   "metadata": {},
   "source": [
    "# 2)regexp_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f6ecf8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------------------+\n",
      "|color_clean|Description                        |\n",
      "+-----------+-----------------------------------+\n",
      "|WHITE      |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|WHITE      |WHITE METAL LANTERN                |\n",
      "|           |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|           |KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|RED        |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+-----------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pulling out the first mentioned color\n",
    "#syntax regexp_extract(column,regex_string_to_replace,capture_group) \n",
    "#0 as capture group means it will extract all the characters in string\n",
    "#If the regex did not match, or the specified group did not match, an empty string is returned\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "df.selectExpr(\"regexp_extract(Description,'(BLACK|WHITE|RED|GREEN|BLUE)',1) as color_clean\",\"Description\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3b292e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n",
      "|InvoiceDate     |reg_exp|\n",
      "+----------------+-------+\n",
      "|01-12-2010 08:26|12     |\n",
      "|01-12-2010 08:26|12     |\n",
      "|01-12-2010 08:26|12     |\n",
      "|01-12-2010 08:26|12     |\n",
      "|01-12-2010 08:26|12     |\n",
      "+----------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "df.selectExpr(\"InvoiceDate\",\"trim(regexp_extract(InvoiceDate,'([0-9]+)(-)([0-9]+)(-)([0-9]+)',3)) as reg_exp\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "57632970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         color_clean|         Description|\n",
      "+--------------------+--------------------+\n",
      "|COLOR HANGING HEA...|WHITE HANGING HEA...|\n",
      "| COLOR METAL LANTERN| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"SELECT regexp_replace(Description, 'BLACK|WHITE|RED|GREEN|BLUE', 'COLOR') as color_clean, \\\n",
    "          Description FROM dfTable\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8d25d8",
   "metadata": {},
   "source": [
    "# 3)translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd7c5c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-----------------------------------+\n",
      "|translate(Description, LEET, 1337) |Description                        |\n",
      "+-----------------------------------+-----------------------------------+\n",
      "|WHI73 HANGING H3AR7 7-1IGH7 HO1D3R |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|WHI73 M37A1 1AN73RN                |WHITE METAL LANTERN                |\n",
      "|CR3AM CUPID H3AR7S COA7 HANG3R     |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|KNI773D UNION F1AG HO7 WA73R BO7713|KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|R3D WOO11Y HO77I3 WHI73 H3AR7.     |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+-----------------------------------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#replace all instances of a character with the indexed character in the replacement string\n",
    "from pyspark.sql.functions import translate\n",
    "df.select(translate(col(\"Description\"), \"LEET\", \"1337\"),col(\"Description\"))\\\n",
    ".show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41644b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------------------+\n",
      "|translate(Description, LEET, 1337)|         Description|\n",
      "+----------------------------------+--------------------+\n",
      "|              WHI73 HANGING H3A...|WHITE HANGING HEA...|\n",
      "|               WHI73 M37A1 1AN73RN| WHITE METAL LANTERN|\n",
      "+----------------------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"SELECT translate(Description, 'LEET', '1337'), Description FROM dfTable\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2fabca98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         Description|\n",
      "+--------------------+\n",
      "|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"SELECT Description FROM dfTable \\\n",
    "WHERE instr(Description, 'BLACK') >= 1 OR instr(Description, 'WHITE') >= 1\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad7aeb4",
   "metadata": {},
   "source": [
    "# working with dates and times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ffad28",
   "metadata": {},
   "source": [
    "# 1) current_date,current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4b137bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------------------+\n",
      "|id |today     |now                   |\n",
      "+---+----------+----------------------+\n",
      "|0  |2022-02-26|2022-02-26 20:08:19.54|\n",
      "|1  |2022-02-26|2022-02-26 20:08:19.54|\n",
      "|2  |2022-02-26|2022-02-26 20:08:19.54|\n",
      "|3  |2022-02-26|2022-02-26 20:08:19.54|\n",
      "|4  |2022-02-26|2022-02-26 20:08:19.54|\n",
      "+---+----------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date,current_timestamp,col\n",
    "dateDF = spark.range(10)\\\n",
    ".withColumn(\"today\", current_date())\\\n",
    ".withColumn(\"now\", current_timestamp())\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")\n",
    "dateDF.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a982f955",
   "metadata": {},
   "source": [
    "# 2) date_add,date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7ea28129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+\n",
      "|date_subtrction|date_addition|\n",
      "+---------------+-------------+\n",
      "|     2022-02-21|   2022-03-03|\n",
      "+---------------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let’s add and subtract five days from today\n",
    "from pyspark.sql.functions import date_add, date_sub\n",
    "dateDF.selectExpr(\"date_sub(today, 5) as date_subtrction\",\"date_add(today, 5) as date_addition\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291d0fa8",
   "metadata": {},
   "source": [
    "# 3) datediff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0cc92df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|date_difference|\n",
      "+---------------+\n",
      "|              7|\n",
      "+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#datediff function that will return the number of days in between two dates\n",
    "from pyspark.sql.functions import datediff, months_between, to_date,lit\n",
    "dateDF.withColumn(\"week_ago\",expr(\"date_sub(today, 7)\"))\\\n",
    ".selectExpr(\"datediff(today,week_ago) as date_difference\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea0c0fc",
   "metadata": {},
   "source": [
    "# 4) to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b9ff13db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     start|       end|\n",
      "+----------+----------+\n",
      "|2016-01-01|2017-05-22|\n",
      "+----------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#months_between gives you the number of months between two dates\n",
    "#The to_date function allows you to convert a string to a date\n",
    "dateDF.selectExpr(\"to_date('2016-01-01') as start\",\n",
    "              \"to_date('2017-05-22') as end\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67712bd2",
   "metadata": {},
   "source": [
    "# 5) month_between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5dc733b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|month_difference|\n",
      "+----------------+\n",
      "|              17|\n",
      "+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.selectExpr(\"to_date('2016-01-01') as start\",\"to_date('2017-05-22') as end\") \\\n",
    ".selectExpr(\"ceil(months_between(end,start)) as month_difference\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "78c1726d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+\n",
      "|      date|mdiff|ddiff|\n",
      "+----------+-----+-----+\n",
      "|2016-01-01| 12.0|  366|\n",
      "|2016-01-01| 12.0|  366|\n",
      "+----------+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"SELECT to_date('2016-01-01') as date, months_between('2017-01-01','2016-01-01') as mdiff,\\\n",
    "          datediff('2017-01-01','2016-01-01') as ddiff FROM dateTable\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1b4b3c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|date1|     date2|\n",
      "+-----+----------+\n",
      "| null|2017-12-11|\n",
      "| null|2017-12-11|\n",
      "+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Spark will not throw an error if it cannot parse the date; rather, it will just return null\n",
    "#To illustrate, let’s take a look at the date format that has switched from year-month-day to \n",
    "#year-day-month.Spark will fail to parse this date and silently return null instead\n",
    "dateDF.select(to_date(lit(\"2016-20-12\")).alias(\"date1\"),to_date(lit(\"2017-12-11\")).alias(\"date2\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9abe598c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-27|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#way to avoid these issues entirely\n",
    "#The first step is to remember that we need to specify our date format according to the JavaSimpleDateFormat standard.\n",
    "dateFormat = \"yyyy-dd-MM\"\n",
    "cleanDateDF = spark.range(1).select(\n",
    "to_date(lit(\"2017-27-11\"), dateFormat).alias(\"date\"),\n",
    "to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n",
    "cleanDateDF.show()\n",
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368936bd",
   "metadata": {},
   "source": [
    "# 6) from_unixtime, unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "02f49e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|10/27/2021|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"SELECT from_unixtime(unix_timestamp('27-10-2021', 'dd-MM-yyyy'),'MM/dd/yyyy') as date, to_date(date2, 'yyyy-dd-MM') as date2 FROM dateTable2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b240c6",
   "metadata": {},
   "source": [
    "# 7) to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b14d911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|to_timestamp(`date`, 'yyyy-dd-MM')|\n",
      "+----------------------------------+\n",
      "|               2017-11-12 00:00:00|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to_timestamp, which always requires a format to be specified:\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "cleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af703bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|to_timestamp(datetable2.`date`, 'yyyy-mm-dd')|\n",
      "+---------------------------------------------+\n",
      "|                          2017-11-12 00:00:00|\n",
      "+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"select to_timestamp(date,'yyyy-mm-dd') from dateTable2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6e0766c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+\n",
      "|CAST(to_date('2017-01-01', 'yyyy-MM-dd') AS TIMESTAMP)|\n",
      "+------------------------------------------------------+\n",
      "|                                   2017-01-01 00:00:00|\n",
      "+------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Casting between dates and timestamps \n",
    "#in sql\n",
    "spark.sql(\"SELECT cast(to_date('2017-01-01', 'yyyy-MM-dd') as timestamp)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3c8fd132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-27|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#comparing dates\n",
    "cleanDateDF.where(\"date2 >'2017-12-12'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200bc21f",
   "metadata": {},
   "source": [
    "# working with null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d731cc5c",
   "metadata": {},
   "source": [
    "# 1) coalesce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff80251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the first non-null value from a set of columns by using the coalesce function.\n",
    "from pyspark.sql.functions import coalesce\n",
    "df.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6b9f8b",
   "metadata": {},
   "source": [
    "# 2) ifnull(), nullIf(), nvl(), nvl2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "97326bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+------------+------------+\n",
      "|      ifnull|nullif|         nvl|        nvl2|\n",
      "+------------+------+------------+------------+\n",
      "|return_value|  null|return_value|return_value|\n",
      "+------------+------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ifnull, nullIf, nvl, and nvl2\n",
    "spark.sql(\"SELECT ifnull(null, 'return_value') as ifnull,nullif('value', 'value') as nullif,nvl(null, 'return_value') as nvl,\\\n",
    "          nvl2('not_null', 'return_value', 'else_value') as nvl2 FROM dfTable LIMIT 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84068b85",
   "metadata": {},
   "source": [
    "# 3) drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "62626961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3108"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "07b1c4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1968"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop, which removes rows that contain nulls.\n",
    "#see the difference between the count values after using drop()\n",
    "df.na.drop().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "38bb6021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|     InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|01-12-2010 08:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|01-12-2010 08:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#In SQL, we have to do this column by column:\n",
    "spark.sql(\"SELECT * FROM dfTable WHERE Description IS NOT NULL\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d9ea1561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Specifying \"any\" as an argument drops a row if any of the values are null. Using “all” drops the\n",
    "#row only if all values are null or NaN for that row:\n",
    "df.na.drop(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "40618a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#can also apply this to certain sets of columns by passing in an array of columns\n",
    "df.na.drop(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381d4993",
   "metadata": {},
   "source": [
    "# 4) fill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bf780fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the fill function, you can fill one or more columns with a set of values\n",
    "#tofill null values of type string\n",
    "df.na.fill(\"All Null values become this string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7fbfac39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to specify null values to specific columns\n",
    "df.na.fill(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ab47bf78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: int, Country: string]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#specifying the fill values of each columns using dictionary\n",
    "fill_cols_vals = {\"StockCode\": 5, \"Description\" : \"No Value\"}\n",
    "df.na.fill(fill_cols_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ed8fcd",
   "metadata": {},
   "source": [
    "# 5) replace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "81e53c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replacing \n",
    "df.na.replace([\"\"], [\"UNKNOWN\"], \"Description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12edace",
   "metadata": {},
   "source": [
    "# Working with Complex Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5eeb20",
   "metadata": {},
   "source": [
    "# 1) struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2e84de6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+---------+---------+-----------------------------------+--------+----------------+---------+----------+--------------+\n",
      "|complex                                      |InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate     |UnitPrice|CustomerID|Country       |\n",
      "+---------------------------------------------+---------+---------+-----------------------------------+--------+----------------+---------+----------+--------------+\n",
      "|[WHITE HANGING HEART T-LIGHT HOLDER, 536365] |536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |01-12-2010 08:26|2.55     |17850     |United Kingdom|\n",
      "|[WHITE METAL LANTERN, 536365]                |536365   |71053    |WHITE METAL LANTERN                |6       |01-12-2010 08:26|3.39     |17850     |United Kingdom|\n",
      "|[CREAM CUPID HEARTS COAT HANGER, 536365]     |536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |01-12-2010 08:26|2.75     |17850     |United Kingdom|\n",
      "|[KNITTED UNION FLAG HOT WATER BOTTLE, 536365]|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |01-12-2010 08:26|3.39     |17850     |United Kingdom|\n",
      "|[RED WOOLLY HOTTIE WHITE HEART., 536365]     |536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |01-12-2010 08:26|3.39     |17850     |United Kingdom|\n",
      "+---------------------------------------------+---------+---------+-----------------------------------+--------+----------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#structs : DataFrames within DataFrames\n",
    "from pyspark.sql.functions import struct\n",
    "df.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"*\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5c035d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "complexDF = df.selectExpr(\"struct(Description,InvoiceNo) as complex\")\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "51abc2a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We now have a DataFrame with a column complex. We can query it just as we might another\n",
    "#DataFrame, the only difference is that we use a dot syntax to do so, or the column method getField\n",
    "complexDF.select(col(\"complex.Description\")).show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ff449361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "| complex.Description|\n",
      "+--------------------+\n",
      "|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN|\n",
      "|CREAM CUPID HEART...|\n",
      "|KNITTED UNION FLA...|\n",
      "|RED WOOLLY HOTTIE...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#other method\n",
    "complexDF.select(col(\"complex\").getField(\"Description\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c22f48cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|         Description|InvoiceNo|\n",
      "+--------------------+---------+\n",
      "|WHITE HANGING HEA...|   536365|\n",
      "| WHITE METAL LANTERN|   536365|\n",
      "|CREAM CUPID HEART...|   536365|\n",
      "|KNITTED UNION FLA...|   536365|\n",
      "|RED WOOLLY HOTTIE...|   536365|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We can also query all values in the struct by using *\n",
    "complexDF.select(\"complex.*\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4e6fe169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|         Description|InvoiceNo|\n",
      "+--------------------+---------+\n",
      "|WHITE HANGING HEA...|   536365|\n",
      "| WHITE METAL LANTERN|   536365|\n",
      "|CREAM CUPID HEART...|   536365|\n",
      "|KNITTED UNION FLA...|   536365|\n",
      "|RED WOOLLY HOTTIE...|   536365|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"SELECT complex.* FROM complexDF\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd1ec4e",
   "metadata": {},
   "source": [
    "# 2) array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3de84be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|split(Description,  , -1)|\n",
      "+-------------------------+\n",
      "|     [WHITE, HANGING, ...|\n",
      "|     [WHITE, METAL, LA...|\n",
      "+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#arrays: take every single word in our Description column using split and convert that into a row in our DataFrame\n",
    "from pyspark.sql.functions import split\n",
    "df.select(split(col(\"Description\"), \" \")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1b19222e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|array_col[0]|array_col[1]|\n",
      "+------------+------------+\n",
      "|       WHITE|     HANGING|\n",
      "|       WHITE|       METAL|\n",
      "|       CREAM|       CUPID|\n",
      "|     KNITTED|       UNION|\n",
      "|         RED|      WOOLLY|\n",
      "+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#can access elements using index\n",
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\\\n",
    ".selectExpr(\"array_col[0]\",\"array_col[1]\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "117d668b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            splitcol|\n",
      "+--------------------+\n",
      "|[WHITE, HANGING, ...|\n",
      "|[WHITE, METAL, LA...|\n",
      "|[CREAM, CUPID, HE...|\n",
      "|[KNITTED, UNION, ...|\n",
      "|[RED, WOOLLY, HOT...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"select split(Description,' ') as splitcol from dfTable\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ad3e2014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|size(split(Description,  , -1))|\n",
      "+-------------------------------+\n",
      "|                              5|\n",
      "|                              3|\n",
      "+-------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use size() to find the array Length\n",
    "from pyspark.sql.functions import size\n",
    "df.select(size(split(\"Description\", ' '))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "46930a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|array_contains|\n",
      "+--------------+\n",
      "|          true|\n",
      "|          true|\n",
      "+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#array_contains : We can also see whether this array contains a value\n",
    "from pyspark.sql.functions import array_contains\n",
    "df.selectExpr(\"array_contains(split(Description,' '),'WHITE') as array_contains\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a1d2ab84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|array_contains(split(Description,  , -1), WHITE)|\n",
      "+------------------------------------------------+\n",
      "|                                            true|\n",
      "|                                            true|\n",
      "+------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"SELECT array_contains(split(Description, ' '), 'WHITE') FROM dfTable\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be6dcc",
   "metadata": {},
   "source": [
    "# 3) explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "84522810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+---------+--------+\n",
      "|Description                       |InvoiceNo|exploded|\n",
      "+----------------------------------+---------+--------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |WHITE   |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HANGING |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HEART   |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |T-LIGHT |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HOLDER  |\n",
      "+----------------------------------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#explode:The explode function takes a column that consists of arrays and creates one row per value in the array\n",
    "from pyspark.sql.functions import split, explode\n",
    "df.withColumn(\"splitted\",split(col(\"Description\"), \" \"))\\\n",
    ".withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",
    ".select(\"Description\", \"InvoiceNo\", \"exploded\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cda42fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------+\n",
      "|         Description|InvoiceNo|exploded|\n",
      "+--------------------+---------+--------+\n",
      "|WHITE HANGING HEA...|   536365|   WHITE|\n",
      "|WHITE HANGING HEA...|   536365| HANGING|\n",
      "|WHITE HANGING HEA...|   536365|   HEART|\n",
      "|WHITE HANGING HEA...|   536365| T-LIGHT|\n",
      "|WHITE HANGING HEA...|   536365|  HOLDER|\n",
      "+--------------------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"select Description,InvoiceNo,explode(split(description,' ')) as exploded from dfTable\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb11634",
   "metadata": {},
   "source": [
    "# 4) posexplode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c7e345f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------------------------------+\n",
      "|pos|col    |Description                       |\n",
      "+---+-------+----------------------------------+\n",
      "|0  |WHITE  |WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|1  |HANGING|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|2  |HEART  |WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|3  |T-LIGHT|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|4  |HOLDER |WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "+---+-------+----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#similiar to explode but returns another columns which specifies the position for each array\n",
    "df.selectExpr(\"posexplode(split(Description,' '))\",\"Description\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdc6b9e",
   "metadata": {},
   "source": [
    "# 5) create_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d1bbfc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|complex_map                                    |\n",
      "+-----------------------------------------------+\n",
      "|[536365 -> WHITE HANGING HEART T-LIGHT HOLDER] |\n",
      "|[536365 -> WHITE METAL LANTERN]                |\n",
      "|[536365 -> CREAM CUPID HEARTS COAT HANGER]     |\n",
      "|[536365 -> KNITTED UNION FLAG HOT WATER BOTTLE]|\n",
      "|[536365 -> RED WOOLLY HOTTIE WHITE HEART.]     |\n",
      "+-----------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#maps: Maps are created by using the map function and key-value pairs of columns\n",
    "from pyspark.sql.functions import create_map\n",
    "df.select(create_map(\"InvoiceNo\",\"Description\").alias(\"complex_map\")).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "885f3ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|complex_map[WHITE METAL LANTERN]|\n",
      "+--------------------------------+\n",
      "|                            null|\n",
      "|                          536365|\n",
      "+--------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#You can query them by using the proper key:check each row for \"WHITE METAL LANTERN\" in the data and return its invoice \n",
    "df.select(create_map(\"Description\",\"InvoiceNo\").alias(\"complex_map\"))\\\n",
    ".selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a135f976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|complex_map                                    |\n",
      "+-----------------------------------------------+\n",
      "|[536365 -> WHITE HANGING HEART T-LIGHT HOLDER] |\n",
      "|[536365 -> WHITE METAL LANTERN]                |\n",
      "|[536365 -> CREAM CUPID HEARTS COAT HANGER]     |\n",
      "|[536365 -> KNITTED UNION FLAG HOT WATER BOTTLE]|\n",
      "|[536365 -> RED WOOLLY HOTTIE WHITE HEART.]     |\n",
      "+-----------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in sql\n",
    "spark.sql(\"select map(InvoiceNo,Description) as complex_map from dfTable\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d1267aca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------+\n",
      "|key                                |value |\n",
      "+-----------------------------------+------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |536365|\n",
      "|WHITE METAL LANTERN                |536365|\n",
      "|CREAM CUPID HEARTS COAT HANGER     |536365|\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|536365|\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |536365|\n",
      "+-----------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#You can also explode map types\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    ".selectExpr(\"explode(complex_map)\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940a1403",
   "metadata": {},
   "source": [
    "# working with json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218d3cc",
   "metadata": {},
   "source": [
    "# 1)to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd27862a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------+\n",
      "|json_string                                                               |\n",
      "+--------------------------------------------------------------------------+\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"} |\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE METAL LANTERN\"}                |\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"CREAM CUPID HEARTS COAT HANGER\"}     |\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"KNITTED UNION FLAG HOT WATER BOTTLE\"}|\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"RED WOOLLY HOTTIE WHITE HEART.\"}     |\n",
      "+--------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#You can turn a StructType into a JSON string by using the to_json function\n",
    "from pyspark.sql.functions import *\n",
    "df1=df.selectExpr(\"to_json(struct(InvoiceNo,Description)) as json_string\")\n",
    "df1.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13990482",
   "metadata": {},
   "source": [
    "# 2) from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d3e2b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "parseSchema = StructType((\n",
    "StructField(\"InvoiceNo\",StringType(),True),\n",
    "StructField(\"Description\",StringType(),True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f828342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|from_json(json_string)                       |\n",
      "+---------------------------------------------+\n",
      "|[536365, WHITE HANGING HEART T-LIGHT HOLDER] |\n",
      "|[536365, WHITE METAL LANTERN]                |\n",
      "|[536365, CREAM CUPID HEARTS COAT HANGER]     |\n",
      "|[536365, KNITTED UNION FLAG HOT WATER BOTTLE]|\n",
      "|[536365, RED WOOLLY HOTTIE WHITE HEART.]     |\n",
      "+---------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#You can use the from_json function to parse the JSON data back in. This naturally requires you to \n",
    "#specify a schema\n",
    "dfp1=df.selectExpr(\"to_json(struct(InvoiceNo,Description)) as json_string\")\n",
    "dfp1.select(from_json(\"json_string\",parseSchema)).show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c652ee9",
   "metadata": {},
   "source": [
    "# user defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "846b87d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user defined functions\n",
    "udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "def power3(double_value):\n",
    "    return double_value ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f73161dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "udfExampleDF.createOrReplaceTempView(\"udf_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d3a7d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to register the function to make it available as a DataFrame function\n",
    "from pyspark.sql.functions import udf\n",
    "power3udf = udf(power3,IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ce793982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|cube_num|\n",
      "+--------+\n",
      "|       0|\n",
      "|       1|\n",
      "|       8|\n",
      "|      27|\n",
      "|      64|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "udfExampleDF.select(power3udf(\"num\").alias(\"cube_num\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "cf0b84ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.power3(double_value)>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#registering udf to work with sql\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "spark.udf.register(\"power3py\", power3, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5b437302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|cube_num|\n",
      "+--------+\n",
      "|       0|\n",
      "|       1|\n",
      "|       8|\n",
      "|      27|\n",
      "|      64|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select power3py(num) as cube_num from udf_df\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
